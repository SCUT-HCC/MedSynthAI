import time
from typing import Dict, Any, List, Optional
from agent_system.recipient import RecipientAgent
from agent_system.triager import TriageAgent
from agent_system.monitor import Monitor
from agent_system.controller import TaskController
from agent_system.prompter import Prompter
from agent_system.inquirer import Inquirer
from agent_system.virtual_patient import VirtualPatientAgent
from agent_system.evaluator import Evaluator
from .task_manager import TaskManager, TaskPhase
from .workflow_logger import WorkflowLogger
from guidance.loader import get_comparison_guidance, _update_guidance_for_Triager


class StepExecutor:
    """
    å•stepæ‰§è¡Œå™¨
    è´Ÿè´£æ‰§è¡Œå•ä¸ªstepä¸­çš„å®Œæ•´agent pipelineæµç¨‹
    """
    
    # å…¨å±€å˜é‡å­˜å‚¨å†å²è¯„åˆ†
    _global_historical_scores = {
        "clinical_inquiry": 0.0,
        "communication_quality": 0.0,
        "information_completeness": 0.0,
        "overall_professionalism": 0.0,
        "present_illness_similarity": 0.0,
        "past_history_similarity": 0.0,
        "chief_complaint_similarity": 0.0
    }
    
    @classmethod
    def reset_historical_scores(cls):
        """é‡ç½®å…¨å±€å†å²è¯„åˆ†"""
        cls._global_historical_scores = {
            "clinical_inquiry": 0.0,
            "communication_quality": 0.0,
            "information_completeness": 0.0,
            "overall_professionalism": 0.0,
            "present_illness_similarity": 0.0,
            "past_history_similarity": 0.0,
            "chief_complaint_similarity": 0.0
        }
    
    def __init__(self, model_type: str = "gpt-oss:latest", llm_config: dict = None, controller_mode: str = "normal"):
        """
        åˆå§‹åŒ–stepæ‰§è¡Œå™¨
        
        Args:
            model_type: ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹ç±»å‹ï¼ˆé™¤Evaluatorå¤–çš„æ‰€æœ‰agentä½¿ç”¨ï¼‰
            llm_config: è¯­è¨€æ¨¡å‹é…ç½®
            controller_mode: ä»»åŠ¡æ§åˆ¶å™¨æ¨¡å¼ï¼Œ'normal'ä¸ºæ™ºèƒ½æ¨¡å¼ï¼Œ'sequence'ä¸ºé¡ºåºæ¨¡å¼ï¼Œ'score_driven'ä¸ºåˆ†æ•°é©±åŠ¨æ¨¡å¼
        
        Note:
            Evaluator agent å›ºå®šä½¿ç”¨ gpt-oss:latest æ¨¡å‹ï¼Œä¸å— model_type å‚æ•°å½±å“
        """
        self.model_type = model_type
        self.llm_config = llm_config or {}
        self.controller_mode = controller_mode
        
        # åˆå§‹åŒ–æ‰€æœ‰agent
        self.recipient = RecipientAgent(model_type=model_type, llm_config=self.llm_config)
        self.triager = TriageAgent(model_type=model_type, llm_config=self.llm_config)
        self.monitor = Monitor(model_type=model_type, llm_config=self.llm_config)
        # æ ¹æ®æ¨¡å¼åˆå§‹åŒ–TaskController
        simple_mode = (controller_mode == "sequence")
        score_driven_mode = (controller_mode == "score_driven")
        self.controller = TaskController(
            model_type=model_type, 
            llm_config=self.llm_config, 
            simple_mode=simple_mode,
            score_driven_mode=score_driven_mode
        )
        self.prompter = Prompter(model_type=model_type, llm_config=self.llm_config)
        self.virtual_patient = VirtualPatientAgent(model_type=model_type, llm_config=self.llm_config)
        # Evaluator å›ºå®šä½¿ç”¨ gpt-oss:latest æ¨¡å‹
        self.evaluator = Evaluator(model_type="gpt-oss:latest", llm_config=self.llm_config)
    
    def execute_step(self, 
                    step_num: int,
                    case_data: Dict[str, Any],
                    task_manager: TaskManager,
                    logger: WorkflowLogger,
                    conversation_history: str = "",
                    previous_hpi: str = "",
                    previous_ph: str = "",
                    previous_chief_complaint: str = "",
                    previous_department: str = "",
                    previous_candidate_department = "",
                    current_guidance: str = "",
                    is_first_step: bool = False,
                    doctor_question: str = "") -> Dict[str, Any]:
        """
        æ‰§è¡Œå•ä¸ªstepçš„å®Œæ•´æµç¨‹
        
        Args:
            step_num: stepç¼–å·
            case_data: ç—…ä¾‹æ•°æ®
            task_manager: ä»»åŠ¡ç®¡ç†å™¨
            logger: æ—¥å¿—è®°å½•å™¨
            conversation_history: å¯¹è¯å†å²
            previous_hpi: ä¸Šè½®ç°ç—…å²
            previous_ph: ä¸Šè½®æ—¢å¾€å²
            previous_chief_complaint: ä¸Šè½®ä¸»è¯‰
            previous_department: ä¸Šè½®åˆ†è¯Šä¸»è¦ç§‘å®¤
            previous_candidate_department: ä¸Šè½®åˆ†è¯Šå€™é€‰ç§‘å®¤
            current_guidance: å½“å‰æŒ‡å¯¼æ–‡æœ¬
            is_first_step: æ˜¯å¦ä¸ºç¬¬ä¸€ä¸ªstep
            doctor_question: åŒ»ç”Ÿé—®é¢˜ï¼ˆéé¦–è½®æ—¶ï¼‰
            
        Returns:
            Dict: stepæ‰§è¡Œç»“æœï¼ŒåŒ…å«æ›´æ–°åçš„ç—…å²ä¿¡æ¯ã€åŒ»ç”Ÿé—®é¢˜ã€æ‚£è€…å›åº”ç­‰
        """
        step_result = {
            "step_number": step_num,
            "success": False,
            "patient_response": "",
            "updated_hpi": previous_hpi,
            "updated_ph": previous_ph,
            "updated_chief_complaint": previous_chief_complaint,
            "triage_result": {
                "primary_department": "",
                "secondary_department": "",
                "triage_reasoning": "",
                "candidate_primary_department": "",
                "candidate_secondary_department": ""
            },
            "doctor_question": "",
            "conversation_history": conversation_history,
            "task_completion_summary": {},
            "errors": []
        }
        
        try:
            # æ›´æ–°ä»»åŠ¡ç®¡ç†å™¨çš„å½“å‰æ­¥éª¤
            task_manager.current_step = step_num
            
            # Step 1: è·å–æ‚£è€…å›åº”
            patient_response = self._get_patient_response(
                step_num, case_data, logger, is_first_step, doctor_question
            )
            step_result["patient_response"] = patient_response
            
            # æ›´æ–°å¯¹è¯å†å²
            if is_first_step:
                updated_conversation = f"æ‚£è€…: {patient_response}"
            else:
                updated_conversation = conversation_history + f"\nåŒ»ç”Ÿ: {doctor_question}\næ‚£è€…: {patient_response}"
            step_result["conversation_history"] = updated_conversation
            
            # Step 2: ä½¿ç”¨Recipientæ›´æ–°ç—…å²ä¿¡æ¯
            recipient_result = self._execute_recipient(
                step_num, logger, updated_conversation, previous_hpi, previous_ph, previous_chief_complaint
            )
            step_result.update({
                "updated_hpi": recipient_result.updated_HPI,
                "updated_ph": recipient_result.updated_PH, 
                "updated_chief_complaint": recipient_result.chief_complaint
            })
            
            # Step 3: ä½¿ç”¨Triagerè¿›è¡Œç§‘å®¤åˆ†è¯Šï¼ˆä»…å½“å½“å‰é˜¶æ®µæ˜¯åˆ†è¯Šé˜¶æ®µæ—¶ï¼‰
            current_phase = task_manager.get_current_phase()
            
            if current_phase == TaskPhase.TRIAGE:
                # å½“å‰å¤„äºåˆ†è¯Šé˜¶æ®µ
                triage_result = self._execute_triager(
                    step_num, logger, recipient_result
                )
                step_result["triage_result"] = {
                    "primary_department": triage_result.primary_department,
                    "secondary_department": triage_result.secondary_department,
                    "triage_reasoning": triage_result.triage_reasoning,
                    "candidate_primary_department": triage_result.candidate_primary_department,
                    "candidate_secondary_department": triage_result.candidate_secondary_department
                }

                department = f"{triage_result.primary_department}-{triage_result.secondary_department}"
                candidate_department = f"{triage_result.candidate_primary_department}-{triage_result.candidate_secondary_department}"
                self.previous_department = department
                self.previous_candidate_department = candidate_department

                # æ ¹æ®é¢„æµ‹ç§‘å®¤åŠ¨æ€æ›´æ–°æŒ‡å¯¼
                new_guidance = self._update_guidance_for_Triager(department)

            else:
                # åˆ†è¯Šå·²å®Œæˆæˆ–å·²è¶…è¿‡åˆ†è¯Šé˜¶æ®µï¼Œä½¿ç”¨å·²æœ‰çš„åˆ†è¯Šç»“æœ
                existing_triage = step_result.get("triage_result", {})
                step_result["triage_result"] = {
                    "primary_department": existing_triage.get("primary_department", "æœªçŸ¥"),
                    "secondary_department": existing_triage.get("secondary_department", "æœªçŸ¥"),
                    "triage_reasoning": existing_triage.get("triage_reasoning", "åˆ†è¯Šå·²å®Œæˆ")
                }
            
            # Step 4: ä½¿ç”¨Monitorè¯„ä¼°ä»»åŠ¡å®Œæˆåº¦
            monitor_results = self._execute_monitor_by_phase(
                step_num, logger, task_manager, recipient_result, step_result.get("triage_result", {})
            )
            
            
            # Step 5: æ›´æ–°ä»»åŠ¡åˆ†æ•°
            self._update_task_scores(step_num, logger, task_manager, monitor_results)
            
            # Step 6: ä½¿ç”¨Controlleré€‰æ‹©ä¸‹ä¸€ä¸ªä»»åŠ¡
            controller_result = self._execute_controller(
                step_num, logger, task_manager, recipient_result
            )
            
            # Step 7: ä½¿ç”¨Prompterç”Ÿæˆè¯¢é—®ç­–ç•¥
            prompter_result = self._execute_prompter(
                step_num, logger, recipient_result, controller_result
            )
            
            # Step 8: ä½¿ç”¨Inquirerç”ŸæˆåŒ»ç”Ÿé—®é¢˜
            doctor_question = self._execute_inquirer(
                step_num, logger, recipient_result, prompter_result
            )
            step_result["doctor_question"] = doctor_question
            
            # Step 9: ä½¿ç”¨Evaluatorè¿›è¡Œè¯„åˆ†
            evaluator_result = self._execute_evaluator(
                step_num, logger, case_data, step_result
            )
            step_result["evaluator_result"] = evaluator_result
            
            # Step 10: è·å–ä»»åŠ¡å®Œæˆæƒ…å†µæ‘˜è¦
            step_result["task_completion_summary"] = task_manager.get_completion_summary()
            
            step_result["success"] = True
            
        except Exception as e:
            error_msg = f"Step {step_num} æ‰§è¡Œå¤±è´¥: {str(e)}"
            step_result["errors"].append(error_msg)
            logger.log_error(step_num, "step_execution_error", error_msg, {"case_data": case_data})
            print(error_msg)
        
        return step_result
    
    def _get_patient_response(self, step_num: int, case_data: Dict[str, Any], 
                             logger: WorkflowLogger, is_first_step: bool, 
                             doctor_question: str = "") -> str:
        """è·å–è™šæ‹Ÿæ‚£è€…çš„å›åº”"""
        start_time = time.time()
        
        try:
            # æ„å»ºè™šæ‹Ÿæ‚£è€…è¾“å…¥
            if is_first_step:
                worker_inquiry = "æ‚¨å¥½ï¼Œè¯·é—®æ‚¨å“ªé‡Œä¸èˆ’æœï¼Ÿ"
            else:
                worker_inquiry = doctor_question
            
            # è°ƒç”¨è™šæ‹Ÿæ‚£è€…agent
            patient_result = self.virtual_patient.run(
                worker_inquiry=worker_inquiry,
                is_first_epoch=is_first_step,
                patient_case=case_data
            )
            
            execution_time = time.time() - start_time
            patient_response = patient_result.current_chat
            
            # è®°å½•æ—¥å¿—
            logger.log_agent_execution(
                step_num, "virtual_patient",
                {
                    "worker_inquiry": worker_inquiry,
                    "is_first_epoch": is_first_step,
                    "case_data": case_data
                },
                {"patient_response": patient_response},
                execution_time
            )
            
            logger.log_patient_response(step_num, patient_response, is_first_step)
            
            return patient_response
            
        except Exception as e:
            error_msg = f"è™šæ‹Ÿæ‚£è€…æ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.log_error(step_num, "virtual_patient_error", error_msg)
            # è¿”å›é»˜è®¤å›åº”
            return "å¯¹ä¸èµ·ï¼Œæˆ‘ä¸å¤ªæ¸…æ¥šæ€ä¹ˆæè¿°ï¼ŒåŒ»ç”Ÿæ‚¨çœ‹ç€åŠå§ã€‚"
    
    def _execute_recipient(self, step_num: int, logger: WorkflowLogger, 
                          conversation_history: str, previous_hpi: str, 
                          previous_ph: str, previous_chief_complaint: str):
        """æ‰§è¡ŒRecipient agent"""
        start_time = time.time()
        
        input_data = {
            "conversation_history": conversation_history,
            "previous_HPI": previous_hpi,
            "previous_PH": previous_ph,
            "previous_chief_complaint": previous_chief_complaint
        }
        
        result = self.recipient.run(**input_data)
        execution_time = time.time() - start_time
        
        output_data = {
            "updated_HPI": result.updated_HPI,
            "updated_PH": result.updated_PH,
            "chief_complaint": result.chief_complaint
        }
        
        logger.log_agent_execution(step_num, "recipient", input_data, output_data, execution_time)
        
        return result
    
    def _execute_triager(self, step_num: int, logger: WorkflowLogger, 
                        recipient_result):
        """æ‰§è¡ŒTriage agentè¿›è¡Œç§‘å®¤åˆ†è¯Š"""
        start_time = time.time()
        
        #æ„å»ºdepartment_comparison_guidance
        comparison_guidance = ""   
        # å¦‚æœå­˜åœ¨ä¸Šä¸€è½®çš„åˆ†è¯Šç»“æœï¼Œå¹¶ä¸”æœ‰ä¸»è¦ç§‘å®¤å’Œå€™é€‰ç§‘å®¤ï¼Œåˆ™ç”Ÿæˆå¯¹æ¯”æŒ‡å¯¼
        if self.previous_department and self.previous_candidate_department:
            comparison_guidance = get_comparison_guidance(previous_department, previous_candidate_department)
        
        # å°†å¯¹æ¯”æŒ‡å¯¼ä¸å¸¸è§„æŒ‡å¯¼åˆå¹¶
            combined_guidance = self.current_guidance
            if comparison_guidance:
                combined_guidance += f"\n\nã€ç§‘å®¤å¯¹æ¯”é‰´åˆ«æŒ‡å¯¼ã€‘\n{comparison_guidance}"

        input_data = {
            "chief_complaint": recipient_result.chief_complaint,
            "hpi_content": recipient_result.updated_HPI,
            "ph_content": recipient_result.updated_PH,
            "combined_guidance": combined_guidance,
        }
        
        result = self.triager.run(**input_data)
        execution_time = time.time() - start_time
        
        output_data = {
            "primary_department": result.primary_department,
            "secondary_department": result.secondary_department,
            "triage_reasoning": result.triage_reasoning,
            "candidate_primary_department": result.candidate_primary_department,
            "candidate_secondary_department": result.candidate_secondary_department,
        }
        #åœ¨æ—¥å¿—ä¸­åŠ å…¥å¯¹æ¯”æŒ‡å¯¼ä¿¡æ¯
        log_input_data = input_data.copy()
        log_input_data["used_comparison_guidance"] = bool(comparison_guidance)
        logger.log_agent_execution(step_num, "triager", input_data, output_data, execution_time)
        
        return result
    
    def _execute_monitor_by_phase(self, step_num: int, logger: WorkflowLogger, 
                                 task_manager: TaskManager, recipient_result, triage_result: Dict[str, Any] = None) -> Dict[str, Dict[str, float]]:
        """æŒ‰é˜¶æ®µæ‰§è¡ŒMonitorè¯„ä¼°ï¼Œåªè¯„ä¼°å½“å‰é˜¶æ®µæœªå®Œæˆçš„ä»»åŠ¡"""
        monitor_results = {}
        current_phase = task_manager.get_current_phase()
        
        # å¦‚æœæ‰€æœ‰ä»»åŠ¡éƒ½å®Œæˆäº†ï¼Œä¸éœ€è¦è¯„ä¼°
        if current_phase == TaskPhase.COMPLETED:
            return monitor_results
        
        # è·å–å½“å‰é˜¶æ®µæœªå®Œæˆçš„ä»»åŠ¡
        pending_tasks = task_manager.get_pending_tasks(current_phase)
        if not pending_tasks:
            return monitor_results
        
        start_time = time.time()
        
        try:
            # ä½¿ç”¨forå¾ªç¯é€ä¸ªè¯„ä¼°æ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
            phase_scores = {}
            for task in pending_tasks:
                task_name = task.get("name", "")
                task_description = task.get("description", "")
                
                # è°ƒç”¨Monitorè¯„ä¼°ç‰¹å®šä»»åŠ¡
                # åˆ†è¯Šé˜¶æ®µä¼ å…¥triage_resultï¼Œå…¶ä»–é˜¶æ®µä¸ä¼ å…¥
                if current_phase == TaskPhase.TRIAGE:
                    # ä½¿ç”¨ä¼ å…¥çš„triage_result
                    monitor_result = self.monitor.run(
                        hpi_content=recipient_result.updated_HPI,
                        ph_content=recipient_result.updated_PH,
                        chief_complaint=recipient_result.chief_complaint,
                        task_name=task_name,
                        task_description=task_description,
                        triage_result=triage_result if triage_result and triage_result.get("primary_department") else None
                    )
                else:
                    # ç°ç—…å²/æ—¢å¾€å²é˜¶æ®µä¸ä¼ å…¥triage_result
                    monitor_result = self.monitor.run(
                        hpi_content=recipient_result.updated_HPI,
                        ph_content=recipient_result.updated_PH,
                        chief_complaint=recipient_result.chief_complaint,
                        task_name=task_name,
                        task_description=task_description
                    )
                
                phase_scores[task_name] = monitor_result.completion_score
                print(f"ä»»åŠ¡'{task_name}'è¯„åˆ†: {monitor_result.completion_score:.2f} - {monitor_result.reason}")
            
            execution_time = time.time() - start_time
            monitor_results[current_phase] = phase_scores
            
            # è®°å½•æ—¥å¿—
            input_data = {
                "hpi_content": recipient_result.updated_HPI,
                "ph_content": recipient_result.updated_PH,
                "chief_complaint": recipient_result.chief_complaint,
                "evaluated_phase": current_phase.value,
                "pending_tasks": [t["name"] for t in pending_tasks]
            }
            
            output_data = {
                "phase_scores": phase_scores,
                "evaluated_tasks": list(phase_scores.keys()),
                "average_score": sum(phase_scores.values()) / len(phase_scores) if phase_scores else 0.0
            }
            
            logger.log_agent_execution(step_num, "monitor", input_data, output_data, execution_time)
            
        except Exception as e:
            error_msg = f"Monitoræ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.log_error(step_num, "monitor_error", error_msg)
            # è¿”å›é»˜è®¤çš„ä½åˆ†è¯„ä¼°
            phase_scores = {task["name"]: 0.1 for task in pending_tasks}
            monitor_results[current_phase] = phase_scores
        
        return monitor_results
    
    def _update_task_scores(self, step_num: int, logger: WorkflowLogger, 
                           task_manager: TaskManager, monitor_results: Dict):
        """æ›´æ–°ä»»åŠ¡åˆ†æ•°"""
        for phase, scores in monitor_results.items():
            if scores:
                old_scores = task_manager.get_task_scores(phase).copy()
                task_manager.update_task_scores(phase, scores)
                new_scores = task_manager.get_task_scores(phase)
                
                logger.log_task_scores_update(step_num, phase.value, old_scores, new_scores)
    
    def _execute_controller(self, step_num: int, logger: WorkflowLogger, 
                           task_manager: TaskManager, recipient_result):
        """æ‰§è¡ŒController agent"""
        start_time = time.time()
        
        # è·å–å½“å‰é˜¶æ®µçš„æœªå®Œæˆä»»åŠ¡
        current_phase = task_manager.get_current_phase()
        pending_tasks = task_manager.get_pending_tasks(current_phase)
        
        input_data = {
            "pending_tasks": pending_tasks,
            "chief_complaint": recipient_result.chief_complaint,
            "hpi_content": recipient_result.updated_HPI,
            "ph_content": recipient_result.updated_PH,
            "task_manager": task_manager  # ä¼ é€’task_managerç”¨äºscore_drivenæ¨¡å¼
        }
        
        result = self.controller.run(**input_data)
        execution_time = time.time() - start_time
        
        # ä¸ºæ—¥å¿—è®°å½•åˆ›å»ºå¯åºåˆ—åŒ–çš„input_dataå‰¯æœ¬ï¼ˆç§»é™¤TaskManagerå¯¹è±¡ï¼‰
        log_input_data = {
            "pending_tasks": input_data["pending_tasks"],
            "chief_complaint": input_data["chief_complaint"],
            "hpi_content": input_data["hpi_content"],
            "ph_content": input_data["ph_content"]
            # ä¸åŒ…å«task_managerï¼Œå› ä¸ºå®ƒä¸èƒ½JSONåºåˆ—åŒ–
        }
        
        output_data = {
            "selected_task": result.selected_task,
            "specific_guidance": result.specific_guidance
        }
        
        logger.log_agent_execution(step_num, "controller", log_input_data, output_data, execution_time)
        
        return result
    
    def _execute_prompter(self, step_num: int, logger: WorkflowLogger, 
                         recipient_result, controller_result):
        """æ‰§è¡ŒPrompter agent"""
        start_time = time.time()
        
        input_data = {
            "hpi_content": recipient_result.updated_HPI,
            "ph_content": recipient_result.updated_PH,
            "chief_complaint": recipient_result.chief_complaint,
            "current_task": controller_result.selected_task,
            "specific_guidance": controller_result.specific_guidance
        }
        
        result = self.prompter.run(**input_data)
        execution_time = time.time() - start_time
        
        output_data = {
            "description": result.description,
            "instructions": result.instructions
        }
        
        logger.log_agent_execution(step_num, "prompter", input_data, output_data, execution_time)
        
        return result
    
    def _execute_inquirer(self, step_num: int, logger: WorkflowLogger, 
                         recipient_result, prompter_result) -> str:
        """æ‰§è¡ŒInquirer agent"""
        start_time = time.time()

        try:
            if new_guidance != self.current_guidance:
                self.current_guidance = new_guidance
        
            # ä½¿ç”¨Prompterç”Ÿæˆçš„æè¿°å’ŒæŒ‡ä»¤åˆå§‹åŒ–Inquirer
            inquirer = Inquirer(
                description=prompter_result.description,
                instructions=prompter_result.instructions,
                model_type=self.model_type,
                llm_config=self.llm_config,
                department_inquiry_guidance=new_guidance,
            )
            # print(f"ğŸ”„ å·²åˆ‡æ¢åˆ° '{first_department}' ç§‘å®¤çš„è¯¢é—®æŒ‡å¯¼")
            
            input_data = {
                "hpi_content": recipient_result.updated_HPI,
                "ph_content": recipient_result.updated_PH,
                "chief_complaint": recipient_result.chief_complaint
            }
            
            result = inquirer.run(**input_data)
            execution_time = time.time() - start_time
            
            doctor_question = result.current_chat
            
            output_data = {"doctor_question": doctor_question}
            
            logger.log_agent_execution(step_num, "inquirer", input_data, output_data, execution_time)
            
            return doctor_question
            
        except Exception as e:
            error_msg = f"Inquireræ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.log_error(step_num, "inquirer_error", error_msg)
            # è¿”å›é»˜è®¤é—®é¢˜
            return "è¯·æ‚¨è¯¦ç»†æè¿°ä¸€ä¸‹æ‚¨çš„ç—‡çŠ¶ï¼ŒåŒ…æ‹¬ä»€ä¹ˆæ—¶å€™å¼€å§‹çš„ï¼Œæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ"

    
    def _execute_evaluator(self, step_num: int, logger: WorkflowLogger, 
                          case_data: Dict[str, Any], step_result: Dict[str, Any]):
        """æ‰§è¡ŒEvaluator agent"""
        start_time = time.time()
        
        try:
            # å‡†å¤‡è¯„ä»·å™¨éœ€è¦çš„æ•°æ®æ ¼å¼ï¼ŒåŒ…å«å®Œæ•´å¯¹è¯å†å²
            conversation_history = step_result.get("conversation_history", "")
            round_data = {
                "patient_response": step_result.get("patient_response", ""),
                "doctor_inquiry": step_result.get("doctor_question", ""),
                "HPI": step_result.get("updated_hpi", ""),
                "PH": step_result.get("updated_ph", ""),
                "chief_complaint": step_result.get("updated_chief_complaint", "")
            }
            
            # ä½¿ç”¨å…¨å±€å†å²è¯„åˆ†
            historical_scores = self._global_historical_scores
            
            # è°ƒç”¨è¯„ä»·å™¨è¿›è¡Œè¯„ä»·ï¼Œä¼ å…¥å®Œæ•´å¯¹è¯å†å²å’Œå†å²è¯„åˆ†
            input_data = {
                "patient_case": case_data,
                "current_round": step_num,
                "round_data": round_data,
                "conversation_history": conversation_history,
                "historical_scores": historical_scores  # æ·»åŠ å†å²è¯„åˆ†ä½œä¸ºæ˜ç¡®å‚æ•°
            }
            
            # æ„å»ºæ‰€æœ‰è½®æ¬¡çš„æ•°æ®ç”¨äºå¤šè½®è¯„ä¼°
            all_rounds_data = []
            
            # ä»å¯¹è¯å†å²ä¸­æå–æ¯è½®æ•°æ®
            lines = conversation_history.strip().split('\n')
            current_round_data = {}
            
            for line in lines:
                line = line.strip()
                if line.startswith('åŒ»ç”Ÿ:') and current_round_data:
                    # å®Œæˆä¸Šè½®ï¼Œå¼€å§‹æ–°è½®
                    all_rounds_data.append(current_round_data)
                    current_round_data = {"doctor_inquiry": line[3:].strip(), "patient_response": ""}
                elif line.startswith('åŒ»ç”Ÿ:'):
                    # æ–°è½®å¼€å§‹
                    current_round_data = {"doctor_inquiry": line[3:].strip(), "patient_response": ""}
                elif line.startswith('æ‚£è€…:') and current_round_data:
                    current_round_data["patient_response"] = line[3:].strip()
                elif line.startswith('æ‚£è€…:'):
                    # ç¬¬ä¸€è½®åªæœ‰æ‚£è€…å›åº”
                    current_round_data = {"doctor_inquiry": "", "patient_response": line[3:].strip()}
            
            # æ·»åŠ æœ€åä¸€è½®
            if current_round_data:
                current_round_data.update({
                    "HPI": step_result.get("updated_hpi", ""),
                    "PH": step_result.get("updated_ph", ""),
                    "chief_complaint": step_result.get("updated_chief_complaint", "")
                })
                all_rounds_data.append(current_round_data)
            
            # ä¸ºæ‰€æœ‰è½®æ¬¡æ·»åŠ evaluation_scoresï¼Œä½¿ç”¨å…¨å±€å†å²è¯„åˆ†
            for i, round_data in enumerate(all_rounds_data):
                if i < step_num - 1:  # å†å²è½®æ¬¡
                    # ä½¿ç”¨å…¨å±€å†å²è¯„åˆ†
                    round_data["evaluation_scores"] = self._global_historical_scores
                else:  # å½“å‰è½®æ¬¡
                    # å½“å‰è½®æ¬¡å°šæœªè¯„åˆ†ï¼Œä½¿ç”¨ç©ºå€¼å ä½
                    round_data["evaluation_scores"] = {
                        "clinical_inquiry": 0.0,
                        "communication_quality": 0.0,
                        "information_completeness": 0.0,
                        "overall_professionalism": 0.0,
                        "present_illness_similarity": 0.0,
                        "past_history_similarity": 0.0,
                        "chief_complaint_similarity": 0.0
                    }
            
            # è°ƒç”¨æ”¯æŒå¤šè½®çš„è¯„ä¼°æ–¹æ³•
            result = self.evaluator.run(
                patient_case=case_data,
                current_round=step_num,
                all_rounds_data=all_rounds_data,
                historical_scores=historical_scores
            )
            
            execution_time = time.time() - start_time
            
            output_data = {
                "clinical_inquiry": {
                    "score": result.clinical_inquiry.score,
                    "comment": result.clinical_inquiry.comment
                },
                "communication_quality": {
                    "score": result.communication_quality.score,
                    "comment": result.communication_quality.comment
                },
                "information_completeness": {
                    "score": result.information_completeness.score,
                    "comment": result.information_completeness.comment
                },
                "overall_professionalism": {
                    "score": result.overall_professionalism.score,
                    "comment": result.overall_professionalism.comment
                },
                "present_illness_similarity": {
                    "score": result.present_illness_similarity.score,
                    "comment": result.present_illness_similarity.comment
                },
                "past_history_similarity": {
                    "score": result.past_history_similarity.score,
                    "comment": result.past_history_similarity.comment
                },
                "chief_complaint_similarity": {
                    "score": result.chief_complaint_similarity.score,
                    "comment": result.chief_complaint_similarity.comment
                },
                "summary": result.summary,
                "key_suggestions": result.key_suggestions
            }
            
            logger.log_agent_execution(step_num, "evaluator", input_data, output_data, execution_time)
            
            # æ›´æ–°å…¨å±€å†å²è¯„åˆ†
            self._global_historical_scores = {
                "clinical_inquiry": result.clinical_inquiry.score,
                "communication_quality": result.communication_quality.score,
                "information_completeness": result.information_completeness.score,
                "overall_professionalism": result.overall_professionalism.score,
                "present_illness_similarity": result.present_illness_similarity.score,
                "past_history_similarity": result.past_history_similarity.score,
                "chief_complaint_similarity": result.chief_complaint_similarity.score
            }
            
            return result
            
        except Exception as e:
            error_msg = f"Evaluatoræ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.log_error(step_num, "evaluator_error", error_msg)
            # è¿”å›é»˜è®¤è¯„ä»·ç»“æœ
            from agent_system.evaluator.response_model import EvaluatorResult, EvaluationDimension
            
            default_dimension = EvaluationDimension(score=0.0, comment="è¯„ä»·å¤±è´¥")
            return EvaluatorResult(
                clinical_inquiry=default_dimension,
                communication_quality=default_dimension,
                information_completeness=default_dimension,
                overall_professionalism=default_dimension,
                present_illness_similarity=default_dimension,
                past_history_similarity=default_dimension,
                chief_complaint_similarity=default_dimension,
                summary="è¯„ä»·å¤±è´¥",
                key_suggestions=["ç³»ç»Ÿéœ€è¦è°ƒè¯•"]
            )