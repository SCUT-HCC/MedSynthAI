import asyncio
import json
import os
import random
from typing import List, Dict, Optional
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai import JsonCssExtractionStrategy
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor

# -------------------------- åŸºç¡€é…ç½® --------------------------
OUTPUT_DIR = "medical_cases_crawl4ai"
os.makedirs(OUTPUT_DIR, exist_ok=True)
ERROR_LOG_PATH = "error_pages_crawl4ai.txt"

# -------------------------- å·¥å…·å‡½æ•° --------------------------
def log_error(page_num: int, error_msg: str):
    with open(ERROR_LOG_PATH, "a", encoding="utf-8") as f:
        f.write(f"Page {page_num}: {error_msg}\n")
    print(f"âŒ ç¬¬ {page_num} é¡µé”™è¯¯ï¼š{error_msg}")

# -------------------------- é€‰æ‹©å™¨é…ç½® --------------------------
CASE_LIST_SCHEMA = {
    "name": "MedicalCaseList",
    "baseSelector": "div.iiyi-list-contn > div.li",
    "fields": [
        {"name": "case_title", "selector": "div.ri > a.t", "type": "text"},
        {"name": "case_url", "selector": "div.ri > a.t", "type": "attribute", "attribute": "href"}
    ]
}

CASE_DETAIL_SCHEMA = {
    "name": "MedicalCaseDetail",
    "baseSelector": "body",
    "fields": [
        {"name": "case_title", "selector": "div.case_details_cont h2", "type": "text"},
        {"name": "primary_department", "selector": "div.breadcrumbs > a:nth-child(2)", "type": "text"},
        {"name": "secondary_department", "selector": "div.breadcrumbs > a:nth-child(3)", "type": "text"},
        {"name": "basic_info", "selector": "div.case_summary.position1 > div.situation > p:nth-child(1) > span", "type": "text"},
        {"name": "html_content1", "selector": "div.case_study.position2", "type": "html"},
        {"name": "html_content2", "selector": "div.case_study.position3", "type": "html"},
        {"name": "html_content3", "selector": "div.case_study.position4", "type": "html"}
    ]
}

# -------------------------- User-Agent æ±  --------------------------
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:127.0) Gecko/20100101 Firefox/127.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36"
]

# -------------------------- æ»‘å—æ£€æµ‹ä¸ç­‰å¾…å‡½æ•° (æ–°é€»è¾‘) --------------------------
async def handle_verification_if_needed(crawler: AsyncWebCrawler, url: str, max_attempts: int = 10) -> bool:
    """
    æ£€æµ‹æ˜¯å¦å­˜åœ¨æ»‘å—éªŒè¯ã€‚å¦‚æœå­˜åœ¨ï¼Œåˆ™ç­‰å¾…ä¸€ä¸ªé€’å¢çš„éšæœºæ—¶é—´åé‡è¯•ã€‚
    """
    if url.startswith("https://"):
        url = url.replace("https://", "http://", 1)

    for attempt in range(1, max_attempts + 1):
        # æ¯æ¬¡å°è¯•éƒ½ä½¿ç”¨æ–°çš„ User-Agent
        crawler.browser_config.user_agent = random.choice(USER_AGENTS)
        print(f"ğŸ”„ [æ»‘å—æ£€æµ‹] ä½¿ç”¨éšæœº User-Agent: {crawler.browser_config.user_agent}")

        config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            page_timeout=30000,
            wait_for="css:body",
        )
        result = await crawler.arun(url=url, config=config)

        if not result.success:
            print(f"âŒ æ— æ³•è®¿é—®è¯¦æƒ…é¡µï¼š{url}ï¼Œé”™è¯¯ï¼š{result.error_message}")
            wait_time = 3600 * attempt + random.uniform(0, 600) # è®¿é—®å¤±è´¥ä¹Ÿå¢åŠ ç­‰å¾…
            print(f"   è®¿é—®å¤±è´¥ï¼Œç­‰å¾… {wait_time / 60:.2f} åˆ†é’Ÿåé‡è¯•...")
            await asyncio.sleep(wait_time)
            continue

        page_content = result.html
        has_verification = (
            "img_verification" in page_content or
            "å›¾å½¢éªŒè¯" in page_content or
            "æ‹–åŠ¨å·¦è¾¹æ»‘å—å®Œæˆä¸Šæ–¹æ‹¼å›¾" in page_content or
            "è¯·å…ˆå®Œæˆå›¾ç‰‡éªŒè¯" in page_content
        )

        if not has_verification:
            print(f"âœ… é¡µé¢æ­£å¸¸ï¼Œæœªæ£€æµ‹åˆ°æ»‘å—éªŒè¯ã€‚")
            return True  # é¡µé¢æ­£å¸¸ï¼Œå¯ä»¥çˆ¬å–

        # å¦‚æœæ£€æµ‹åˆ°æ»‘å—
        print(f"âš ï¸ æ£€æµ‹åˆ°æ»‘å—éªŒè¯ (ç¬¬ {attempt}/{max_attempts} æ¬¡å°è¯•)ã€‚URL: {url}")
        if attempt < max_attempts:
            # é€’å¢ç­‰å¾…æ—¶é—´ï¼š1å°æ—¶ã€2å°æ—¶ã€3å°æ—¶... æ¯æ¬¡å†åŠ ä¸€ä¸ªéšæœºæ•°
            wait_time = 3600 * attempt + random.uniform(0, 1800)
            print(f"   å°†åœ¨ {wait_time / 3600:.2f} å°æ—¶åå†æ¬¡å°è¯•...")
            await asyncio.sleep(wait_time)
        else:
            print(f"âŒ å·²è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•° ({max_attempts}æ¬¡)ï¼Œä»å­˜åœ¨æ»‘å—éªŒè¯ã€‚æ”¾å¼ƒè¯¥ URLã€‚")
            return False # è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒ

    print(f"âŒ ç»è¿‡ {max_attempts} æ¬¡å°è¯•åï¼Œä»æ— æ³•æ­£å¸¸è®¿é—®é¡µé¢ã€‚æ”¾å¼ƒè¯¥ URLã€‚")
    return False

# -------------------------- åˆ—è¡¨é¡µçˆ¬å– --------------------------
async def extract_case_links(crawler: AsyncWebCrawler, page_url: str) -> List[Dict]:
    # æ¯æ¬¡è¯·æ±‚éƒ½ä½¿ç”¨æ–°çš„ User-Agent
    crawler.browser_config.user_agent = random.choice(USER_AGENTS)
    print(f"ğŸ”„ [åˆ—è¡¨é¡µ] ä½¿ç”¨éšæœº User-Agent: {crawler.browser_config.user_agent}")

    config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        extraction_strategy=JsonCssExtractionStrategy(schema=CASE_LIST_SCHEMA),
        wait_for="css:div.iiyi-list-contn > div.li",
        page_timeout=30000
    )
    result = await crawler.arun(url=page_url, config=config)
    if not result.success:
        raise Exception(f"åˆ—è¡¨é¡µçˆ¬å–å¤±è´¥ï¼š{result.error_message}")

    try:
        case_links = json.loads(result.extracted_content)
        from urllib.parse import urljoin
        for case in case_links:
            if case.get("case_url"):
                case["case_url"] = urljoin(page_url, case["case_url"])
        return case_links
    except json.JSONDecodeError:
        raise Exception(f"é“¾æ¥è§£æå¤±è´¥ï¼š{result.extracted_content[:200]}...")

# -------------------------- è¯¦æƒ…é¡µçˆ¬å– --------------------------
async def extract_case_details(crawler: AsyncWebCrawler, case_url: str, referer_url: str) -> Optional[Dict]:
    """
    çˆ¬å–è¯¦æƒ…é¡µæ•°æ®
    æ–°å¢ referer_url å‚æ•°æ¥ä¼ªé€  Referer è¯·æ±‚å¤´
    """
    if case_url.startswith("https://"):
        case_url = case_url.replace("https://", "http://", 1)

    # æ¯æ¬¡è¯·æ±‚éƒ½ä½¿ç”¨æ–°çš„ User-Agent
    crawler.browser_config.user_agent = random.choice(USER_AGENTS)
    print(f"ğŸ”„ [è¯¦æƒ…é¡µ] ä½¿ç”¨éšæœº User-Agent: {crawler.browser_config.user_agent}")

    config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        extraction_strategy=JsonCssExtractionStrategy(schema=CASE_DETAIL_SCHEMA),
        wait_for="css:div.case_study",
        page_timeout=60000,
    )

    try:
        result = await crawler.arun(
            url=case_url,
            config=config,
            headers={"Referer": referer_url}
        )
        if not result.success:
            print(f"âš ï¸ è¯¦æƒ…é¡µ {case_url} çˆ¬å–å¤±è´¥ï¼š{result.error_message}")
            return None

        extracted_data = json.loads(result.extracted_content)
        raw_detail = extracted_data[0] if isinstance(extracted_data, list) and extracted_data else extracted_data if isinstance(extracted_data, dict) else None
        if not raw_detail:
            print(f"âš ï¸ è¯¦æƒ…é¡µ {case_url}ï¼šæå–ç»“æœæ ¼å¼å¼‚å¸¸æˆ–ä¸ºç©º")
            return None

        raw_detail["case_url"] = case_url
        html_content1 = raw_detail.get("html_content1", "")
        html_content2 = raw_detail.get("html_content2", "")
        html_content3 = raw_detail.get("html_content3", "")

        target_case_struct = {
            "case_url": raw_detail.get("case_url", ""),
            "case_title": raw_detail.get("case_title", "").strip(),
            "primary_department": raw_detail.get("primary_department", "").strip(),
            "secondary_department": raw_detail.get("secondary_department", "").strip(),
            "basic_info": raw_detail.get("basic_info", "").strip(),
            "case_introduction": {},
            "diagnosis_treatment_process": {},
            "analysis_summary": ""
        }

        for html_content in [html_content1, html_content2, html_content3]:
            if not html_content: continue
            soup = BeautifulSoup(html_content, "html.parser")
            for section in soup.select("div.case_study"):
                section_title_element = section.select_one("h2")
                if not section_title_element: continue
                section_name = section_title_element.get_text(strip=True)

                target_field = None
                if "ç—…æ¡ˆä»‹ç»" in section_name: target_field = target_case_struct["case_introduction"]
                elif "è¯Šæ²»è¿‡ç¨‹" in section_name: target_field = target_case_struct["diagnosis_treatment_process"]
                elif "åˆ†ææ€»ç»“" in section_name:
                    target_case_struct["analysis_summary"] = section.get_text(strip=True).replace(section_name, "", 1).strip()
                    continue
                else: continue

                for sub_section in section.select("div"):
                    sub_title_element = sub_section.select_one("h3 > em")
                    if not sub_title_element: continue
                    sub_title_name = sub_title_element.get_text(strip=True)
                    sub_content = sub_section.get_text(strip=True).replace(sub_title_name, "", 1).strip()
                    
                    field_map = {
                        "ä¸»è¯‰": "chief_complaint", "ç°ç—…å²": "history_of_present_illness",
                        "æ—¢å¾€å²": "past_medical_history", "æŸ¥ä½“": "physical_examination",
                        "è¾…åŠ©æ£€æŸ¥": "auxiliary_examinations", "åˆæ­¥è¯Šæ–­": "preliminary_diagnosis",
                        "è¯Šæ–­ä¾æ®": "diagnostic_basis", "é‰´åˆ«è¯Šæ–­": "differential_diagnosis",
                        "è¯Šæ²»ç»è¿‡": "treatment_course", "è¯Šæ–­ç»“æœ": "final_diagnosis"
                    }
                    for key, value in field_map.items():
                        if key in sub_title_name:
                            target_field[value] = sub_content
                            break
        return target_case_struct
    except Exception as e:
        print(f"âš ï¸ è¯¦æƒ…é¡µ {case_url} è§£æå¤±è´¥ï¼š{str(e)}")
        return None

# -------------------------- å•é¡µçˆ¬å– (æ–°é€»è¾‘) --------------------------
async def crawl_single_page(crawler: AsyncWebCrawler, page_num: int, total_pages: int) -> List[Dict]:
    """çˆ¬å–å•é¡µæ•°æ®ï¼Œä¸ä½¿ç”¨ä»£ç†"""
    page_url = f"http://www.iiyi.com/?a=b&page={page_num}"
    print(f"\nğŸ“„ æ­£åœ¨çˆ¬å–åˆ—è¡¨é¡µ {page_num}/{total_pages}ï¼š{page_url}")

    try:
        case_links = await extract_case_links(crawler, page_url)
    except Exception as e:
        log_error(page_num, str(e))
        return []

    print(f"âœ… ç¬¬ {page_num} é¡µå‘ç° {len(case_links)} ä¸ªç—…ä¾‹")
    if not case_links:
        log_error(page_num, "æœªæå–åˆ°ä»»ä½•ç—…ä¾‹é“¾æ¥")
        return []

    page_cases = []
    for idx, case in enumerate(case_links, 1):
        case_url = case.get("case_url")
        if not case_url: continue

        # --- æ¯ä¸ªè¯¦æƒ…é¡µè¯·æ±‚å‰çš„éšæœºå»¶è¿Ÿ ---
        request_delay = random.uniform(8, 15)
        print(f"   ...ç­‰å¾… {request_delay:.2f} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªç—…ä¾‹...")
        await asyncio.sleep(request_delay)

        print(f"æ­£åœ¨å¤„ç†ç¬¬ {idx}/{len(case_links)} ä¸ªç—…ä¾‹ï¼š{case_url}")
        try:
            # æ£€æµ‹æ»‘å—éªŒè¯ï¼Œå¦‚æœå¤±è´¥åˆ™è·³è¿‡
            if not await handle_verification_if_needed(crawler, case_url):
                print(f"âš ï¸ æ»‘å—éªŒè¯æœ€ç»ˆå¤±è´¥ï¼Œè·³è¿‡è¯¥ç—…ä¾‹ï¼š{case_url}")
                continue

            # æ»‘å—éªŒè¯é€šè¿‡ï¼Œç»§ç»­çˆ¬å–
            # æ³¨æ„ï¼šä¼ å…¥äº† page_url ä½œä¸º referer
            case_detail = await extract_case_details(crawler, case_url, referer_url=page_url)
            if case_detail:
                page_cases.append(case_detail)
                print(f"âœ… æˆåŠŸæå–ç—…ä¾‹ï¼š{case_detail.get('case_title', 'æœªçŸ¥æ ‡é¢˜')}")
            else:
                print(f"âš ï¸ è¯¦æƒ…é¡µæå–å¤±è´¥ï¼š{case_url}")
        except Exception as e:
            print(f"âŒ å¤„ç†è¯¦æƒ…é¡µæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯ï¼š{str(e)}")
            continue

    return page_cases

# -------------------------- ä¸»çˆ¬å–é€»è¾‘ (æ–°é€»è¾‘) --------------------------
async def main(start_page: int, end_page: int):
    browser_config = BrowserConfig(
        browser_type="chromium",
        headless=True,
        user_agent=random.choice(USER_AGENTS),
        # 3. æ–°å¢ï¼šå¼€å¯ Stealth æ¨¡å¼
        # stealth=True
    )

    all_cases = []
    async with AsyncWebCrawler(config=browser_config) as crawler:
        for page_num in range(start_page, end_page + 1):
            # ä¹‹å‰åœ¨è¿™é‡Œæ›´æ–°UAçš„é€»è¾‘å·²è¢«ç§»åˆ°å…·ä½“è¯·æ±‚å‡½æ•°ä¸­

            page_cases = await crawl_single_page(crawler, page_num, end_page)
            all_cases.extend(page_cases)

            # --- æ¯ä¸ªåˆ—è¡¨é¡µçˆ¬å–å®Œæˆåçš„é•¿å»¶è¿Ÿ ---
            if page_num < end_page:
                page_delay = random.uniform(60, 120)
                print(f"\n--- ç¬¬ {page_num} é¡µå¤„ç†å®Œæ¯•ï¼Œä¼‘æ¯ {page_delay:.2f} ç§’åè¿›å…¥ä¸‹ä¸€é¡µ ---")
                await asyncio.sleep(page_delay)

    if all_cases:
        summary_path = os.path.join(OUTPUT_DIR, "all_cases_summary.json")
        with open(summary_path, "w", encoding="utf-8") as f:
            json.dump(all_cases, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼å…±è·å– {len(all_cases)} ä¸ªæœ‰æ•ˆç—…ä¾‹ï¼Œæ±‡æ€»æ–‡ä»¶ï¼š{summary_path}")
    else:
        print(f"\nâš ï¸  çˆ¬å–å®Œæˆï¼Œä½†æœªè·å–æœ‰æ•ˆç—…ä¾‹")

# -------------------------- å¯åŠ¨å…¥å£ --------------------------
if __name__ == "__main__":
    TARGET_START_PAGE = 2
    TARGET_END_PAGE = 1000
    asyncio.run(main(start_page=TARGET_START_PAGE, end_page=TARGET_END_PAGE))