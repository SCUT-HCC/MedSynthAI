"""
çˆ±çˆ±åŒ»ç—…å†æ•°æ®é‡‡é›†æ¨¡å— (æ”¹è¿›ç‰ˆ)

ä½¿ç”¨ crawl4ai çš„ JsonCssExtractionStrategy å’Œ PruningContentFilter
è¿›è¡Œç²¾ç¡®çš„ç»“æ„åŒ–æ•°æ®æå–ã€‚

ä¸»è¦æ”¹è¿›:
1. ä½¿ç”¨ CSS é€‰æ‹©å™¨è¿›è¡Œç²¾ç¡®æ•°æ®æå–
2. ç»“åˆ PruningContentFilter ä¼˜åŒ–å†…å®¹è´¨é‡
3. ç”Ÿæˆç»“æ„åŒ–çš„ JSON æ•°æ®

åŠŸèƒ½æ¨¡å—:
    1. URLé‡‡é›†æ¨¡å— (fetch_all_case_urls) - ä¿æŒä¸å˜
    2. ç—…ä¾‹è¯¦æƒ…çˆ¬å–æ¨¡å— (crawl_case_details_improved)
       - ä½¿ç”¨ JsonCssExtractionStrategy è¿›è¡Œç»“æ„åŒ–æå–
       - ä½¿ç”¨ PruningContentFilter ä¼˜åŒ–å†…å®¹è´¨é‡
       - ç›´æ¥ä¿å­˜ç»“æ„åŒ–çš„ JSON æ•°æ®

ä½¿ç”¨ç¤ºä¾‹:
    # æ”¹è¿›çš„å®Œæ•´æµç¨‹
    asyncio.run(main_improved())

    # ä»…é‡‡é›†URL
    asyncio.run(main_fetch_urls())

    # ä»…çˆ¬å–è¯¦æƒ…ï¼ˆä½¿ç”¨æ”¹è¿›ç‰ˆï¼‰
    asyncio.run(main_crawl_details_improved())

è¾“å‡ºè¯´æ˜:
    - iiyi_case_urls.txt: ç—…å†URLåˆ—è¡¨æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªURLï¼‰
    - case_details/: ç—…ä¾‹JSONæ–‡ä»¶ç›®å½•ï¼ˆæ¯ä¸ªç—…ä¾‹ä¸€ä¸ª.jsonæ–‡ä»¶ï¼‰
"""

import asyncio
import re
import json
import os
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Union, Optional, Set
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai.content_filter_strategy import PruningContentFilter
from bs4 import BeautifulSoup


# çˆ±çˆ±åŒ»ç—…å†ç›¸å…³URLé…ç½®
LIST_PAGE_BASE = "https://www.iiyi.com/"  # ç—…å†åˆ—è¡¨é¡µåŸºç¡€URL
CASE_DETAIL_BASE = "https://bingli.iiyi.com/show"  # ç—…å†è¯¦æƒ…é¡µåŸºç¡€URL
LIST_PAGE_PATTERN = "https://www.iiyi.com/?a=b&p={page}"  # åˆ—è¡¨é¡µURLæ¨¡å¼


# ========== æ”¹è¿›çš„æ•°æ®æå–Schema ==========
def get_case_extraction_schema() -> Dict:
    """
    è·å–ç—…ä¾‹è¯¦æƒ…æå–çš„JSON Schema
    
    åŸºäºå¯¹HTMLç»“æ„çš„åˆ†æï¼Œè®¾è®¡ç²¾ç¡®çš„CSSé€‰æ‹©å™¨æ¥æå–ï¼š
    - å‘å¸ƒäººä¿¡æ¯
    - ç—…ä¾‹æ‘˜è¦
    - ç—…æ¡ˆä»‹ç»
    - è¯Šæ²»è¿‡ç¨‹
    - åˆ†ææ€»ç»“
    """
    
    schema = {
        "name": "çˆ±çˆ±åŒ»ç—…ä¾‹è¯¦æƒ…",
        "baseSelector": "body",  # æ•´ä¸ªé¡µé¢ä½œä¸ºåŸºç¡€
        "fields": [
            {
                "name": "title", 
                "selector": "h2", 
                "type": "text"
            },
            {
                "name": "publisher_info", 
                "selector": ".doctor_desc", 
                "type": "text",
                "transformers": ["clean_text"]
            },
            {
                "name": "publisher_name", 
                "selector": ".doctor_desc span", 
                "type": "text"
            },
            {
                "name": "publisher_title", 
                "selector": ".doctor_desc i", 
                "type": "text"
            },
            {
                "name": "publisher_update_time", 
                "selector": ".doctor_desc p:last-child", 
                "type": "text"
            },
            {
                "name": "case_summary", 
                "selector": ".case_summary.position1", 
                "type": "text",
                "transformers": ["clean_text"]
            },
            {
                "name": "case_summary_structured", 
                "selector": ".case_summary.position1 .situation p", 
                "type": "multiple",
                "fields": [
                    {
                        "name": "label", 
                        "selector": "var", 
                        "type": "text"
                    },
                    {
                        "name": "content", 
                        "selector": "span", 
                        "type": "text"
                    }
                ]
            },
            {
                "name": "case_introduction", 
                "selector": ".case_study.position2", 
                "type": "text",
                "transformers": ["clean_text"]
            },
            {
                "name": "diagnosis_treatment", 
                "selector": ".case_study.position3", 
                "type": "text",
                "transformers": ["clean_text"]
            },
            {
                "name": "analysis_summary", 
                "selector": ".case_study.position4", 
                "type": "text",
                "transformers": ["clean_text"]
            },
            {
                "name": "tags", 
                "selector": ".doctors_excel a.on span", 
                "type": "text"
            },
            {
                "name": "department", 
                "selector": ".breadcrumbs a:last-child", 
                "type": "text"
            },
            {
                "name": "images", 
                "selector": ".case_focus_map img", 
                "type": "multiple",
                "fields": [
                    {
                        "name": "src", 
                        "selector": "", 
                        "type": "attribute", 
                        "attribute": "src"
                    },
                    {
                        "name": "alt", 
                        "selector": "", 
                        "type": "attribute", 
                        "attribute": "alt"
                    }
                ]
            }
        ]
    }
    
    return schema


def get_simple_case_extraction_schema() -> Dict:
    """
    ç®€åŒ–çš„ç—…ä¾‹æå–Schemaï¼Œå¤„ç†å¯èƒ½çš„é€‰æ‹©å™¨å˜åŒ–
    """
    
    schema = {
        "name": "çˆ±çˆ±åŒ»ç—…ä¾‹è¯¦æƒ…_ç®€åŒ–ç‰ˆ",
        "baseSelector": "body",
        "fields": [
            {
                "name": "page_title", 
                "selector": "title", 
                "type": "text"
            },
            {
                "name": "case_title", 
                "selector": "h2", 
                "type": "text"
            },
            {
                "name": "publisher_info", 
                "selector": ".doctor_desc, .doctor_desc_left", 
                "type": "text"
            },
            {
                "name": "case_summary", 
                "selector": ".case_summary, .case_summary.position1, .situation", 
                "type": "text"
            },
            {
                "name": "case_content", 
                "selector": ".case_study, .case_study.position2, .case_study.position3, .case_study.position4", 
                "type": "text"
            },
            {
                "name": "main_content", 
                "selector": ".case_details_left, .case_details_cont", 
                "type": "text"
            }
        ]
    }
    
    return schema


# ========== URLè·å–å‡½æ•° (ä¿æŒä¸å˜) ==========

async def fetch_all_case_urls(
    start_page: int = 1,
    end_page: Optional[int] = None,
    max_pages: int = 100,
    verbose: bool = True
) -> List[str]:
    """
    è·å–çˆ±çˆ±åŒ»ç½‘ç«™çš„æ‰€æœ‰ç—…å†URL - ä¿æŒåŸæœ‰é€»è¾‘ä¸å˜
    """
    # ... (ä¿æŒåŸæœ‰çš„fetch_all_case_urlså‡½æ•°ä»£ç )
    if verbose:
        print("ğŸ” å¼€å§‹è·å–çˆ±çˆ±åŒ»ç—…å† URL...")

    case_urls: Set[str] = set()

    # ========== ç¬¬ä¸€é˜¶æ®µï¼šç¡®å®šé¡µé¢èŒƒå›´ ==========
    if end_page is None:
        if verbose:
            print("ğŸ” è‡ªåŠ¨æ¢æµ‹æœ€åä¸€é¡µ...")
        end_page = await _detect_last_page(start_page, max_pages, verbose)
        if verbose:
            print(f"âœ… æ£€æµ‹åˆ°æœ€åä¸€é¡µ: ç¬¬ {end_page} é¡µ")

    # é™åˆ¶æœ€å¤§é¡µæ•°
    if end_page - start_page + 1 > max_pages:
        if verbose:
            print(f"âš ï¸ é¡µé¢èŒƒå›´è¶…è¿‡æœ€å¤§é™åˆ¶ {max_pages}ï¼Œå°†åªçˆ¬å–å‰ {max_pages} é¡µ")
        end_page = start_page + max_pages - 1

    total_pages = end_page - start_page + 1
    if verbose:
        print(f"ğŸ“„ å°†çˆ¬å– {total_pages} ä¸ªåˆ—è¡¨é¡µ (ç¬¬ {start_page} é¡µåˆ°ç¬¬ {end_page} é¡µ)")

    # ========== ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡çˆ¬å–åˆ—è¡¨é¡µ ==========
    async with AsyncWebCrawler() as crawler:
        # ç”Ÿæˆæ‰€æœ‰åˆ—è¡¨é¡µURL
        list_page_urls = [
            LIST_PAGE_PATTERN.format(page=page)
            for page in range(start_page, end_page + 1)
        ]

        # é…ç½®çˆ¬è™«
        crawl_config = CrawlerRunConfig(
            only_text=False,
            verbose=verbose
        )

        if verbose:
            print(f"\nğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(list_page_urls)} ä¸ªåˆ—è¡¨é¡µ...")

        # æ‰¹é‡çˆ¬å–æ‰€æœ‰åˆ—è¡¨é¡µ
        results = await crawler.arun_many(list_page_urls, config=crawl_config)

        # ========== ç¬¬ä¸‰é˜¶æ®µï¼šæå–ç—…å†é“¾æ¥ ==========
        page_count = 0
        for result in results:
            page_count += 1

            if not result.success:
                if verbose:
                    print(f"âš ï¸ ç¬¬ {page_count} é¡µçˆ¬å–å¤±è´¥: {result.url}")
                continue

            # ä»HTMLä¸­æå–æ‰€æœ‰ç—…å†è¯¦æƒ…é¡µé“¾æ¥
            case_links = _extract_case_urls_from_html(result.html)
            case_urls.update(case_links)

            if verbose:
                print(f"âœ“ ç¬¬ {page_count}/{total_pages} é¡µ: å‘ç° {len(case_links)} ä¸ªç—…å†é“¾æ¥ "
                      f"(ç´¯è®¡ {len(case_urls)} ä¸ª)")

    # ========== ç¬¬å››é˜¶æ®µï¼šè½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº ==========
    final_urls = sorted(list(case_urls))

    if verbose:
        print(f"\nâœ… å®Œæˆï¼å…±å‘ç° {len(final_urls)} ä¸ªå”¯ä¸€ç—…å† URL")

    return final_urls


async def _detect_last_page(
    start_page: int = 1,
    max_pages: int = 100,
    verbose: bool = False
) -> int:
    """æ£€æµ‹æœ€åä¸€é¡µ - ä¿æŒåŸæœ‰é€»è¾‘"""
    async def _page_has_cases(page_num: int) -> bool:
        """æ£€æŸ¥æŒ‡å®šé¡µç æ˜¯å¦åŒ…å«ç—…å†"""
        url = LIST_PAGE_PATTERN.format(page=page_num)

        async with AsyncWebCrawler() as crawler:
            config = CrawlerRunConfig(verbose=False)
            result = await crawler.arun(url, config=config)

            if not result.success:
                return False

            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç—…å†é“¾æ¥
            case_links = _extract_case_urls_from_html(result.html)
            return len(case_links) > 0

    # äºŒåˆ†æŸ¥æ‰¾æœ€åä¸€é¡µ
    left = start_page
    right = start_page + max_pages
    last_valid_page = start_page

    while left <= right:
        mid = (left + right) // 2

        if verbose:
            print(f"  æ£€æŸ¥ç¬¬ {mid} é¡µ...")

        has_cases = await _page_has_cases(mid)

        if has_cases:
            last_valid_page = mid
            left = mid + 1
        else:
            right = mid - 1

    return last_valid_page


def _extract_case_urls_from_html(html: str) -> List[str]:
    """ä»HTMLä¸­æå–ç—…å†URL - ä¿æŒåŸæœ‰é€»è¾‘"""
    case_urls: Set[str] = set()

    # åŒ¹é…å„ç§å¯èƒ½çš„URLæ ¼å¼
    patterns = [
        r'https?://bingli\.iiyi\.com/show/[^"\'<>\s]+\.html',
        r'//bingli\.iiyi\.com/show/[^"\'<>\s]+\.html',
        r'/show/[^"\'<>\s]+\.html',
    ]

    for pattern in patterns:
        matches = re.findall(pattern, html)
        for match in matches:
            # è§„èŒƒåŒ–URL
            if match.startswith('//'):
                url = 'https:' + match
            elif match.startswith('/show/'):
                url = 'https://bingli.iiyi.com' + match
            else:
                url = match

            # éªŒè¯URLæ ¼å¼ï¼šå¿…é¡»åŒ…å«"-"
            filename_match = re.search(r'/show/([^/]+)\.html', url)
            if filename_match:
                filename = filename_match.group(1)
                if '-' in filename:
                    case_urls.add(url)

    return list(case_urls)


async def save_case_urls_to_file(
    urls: List[str],
    output_file: str = "iiyi_case_urls.txt"
) -> None:
    """ä¿å­˜URLåˆ°æ–‡ä»¶ - ä¿æŒåŸæœ‰é€»è¾‘"""
    with open(output_file, "w", encoding="utf-8") as f:
        for url in urls:
            f.write(f"{url}\n")

    print(f"ğŸ’¾ å·²ä¿å­˜ {len(urls)} ä¸ª URL åˆ° {output_file}")


def _load_urls_from_file(url_file: str) -> List[str]:
    """ä»æ–‡ä»¶åŠ è½½URL - ä¿æŒåŸæœ‰é€»è¾‘"""
    with open(url_file, "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f if line.strip()]

    return urls


def _extract_case_id_from_url(url: str) -> str:
    """ä»URLæå–ç—…ä¾‹ID - ä¿æŒåŸæœ‰é€»è¾‘"""
    match = re.search(r'/show/([^/]+)\.html', url)
    if match:
        return match.group(1)
    return str(hash(url))


# ========== æ”¹è¿›çš„æ•°æ®æå–å‡½æ•° ==========

def _create_content_filter():
    """åˆ›å»ºä¼˜åŒ–çš„å†…å®¹è¿‡æ»¤å™¨"""
    return PruningContentFilter(
        threshold=0.45,           # å†…å®¹å¯†åº¦é˜ˆå€¼
        threshold_type="dynamic", # åŠ¨æ€é˜ˆå€¼
        min_word_threshold=3      # æœ€å°‘è¯æ•°
    )


def _clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬å†…å®¹"""
    if not text:
        return ""
    
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text.strip())
    # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    return text


def _extract_publisher_from_structured_data(data: Dict) -> str:
    """ä»ç»“æ„åŒ–æ•°æ®ä¸­æå–å‘å¸ƒäººä¿¡æ¯"""
    publisher_parts = []
    
    # æå–å§“å
    if 'publisher_name' in data and data['publisher_name']:
        publisher_parts.append(data['publisher_name'])
    
    # æå–èŒç§°
    if 'publisher_title' in data and data['publisher_title']:
        publisher_parts.append(data['publisher_title'])
    
    # æå–æ›´æ–°æ—¶é—´
    if 'publisher_update_time' in data and data['publisher_update_time']:
        time_match = re.search(r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2})', data['publisher_update_time'])
        if time_match:
            publisher_parts.append(f"æ›´æ–°æ—¶é—´ï¼š{time_match.group(1)}")
    
    return " | ".join(publisher_parts) if publisher_parts else "å‘å¸ƒäººä¿¡æ¯æå–å¤±è´¥"


def _format_case_summary_structured(data: Dict) -> str:
    """æ ¼å¼åŒ–ç»“æ„åŒ–çš„ç—…ä¾‹æ‘˜è¦"""
    summary_parts = []
    
    # å¤„ç†ç»“æ„åŒ–çš„ç—…ä¾‹æ‘˜è¦
    if 'case_summary_structured' in data and data['case_summary_structured']:
        for item in data['case_summary_structured']:
            if isinstance(item, dict) and 'label' in item and 'content' in item:
                summary_parts.append(f"{item['label']} {item['content']}")
    
    # å¦‚æœæ²¡æœ‰ç»“æ„åŒ–æ•°æ®ï¼Œå°è¯•ä»æ™®é€šæ–‡æœ¬ä¸­æå–
    if not summary_parts and 'case_summary' in data:
        summary_text = data['case_summary']
        # å°è¯•æå–å…³é”®ä¿¡æ¯
        patterns = {
            'åŸºæœ¬ä¿¡æ¯': r'ã€åŸºæœ¬ä¿¡æ¯ã€‘([^ã€]+)',
            'å‘ç—…åŸå› ': r'ã€å‘ç—…åŸå› ã€‘([^ã€]+)',
            'ä¸´åºŠè¯Šæ–­': r'ã€ä¸´åºŠè¯Šæ–­ã€‘([^ã€]+)',
            'æ²»ç–—æ–¹æ¡ˆ': r'ã€æ²»ç–—æ–¹æ¡ˆã€‘([^ã€]+)',
            'æ²»ç–—ç»“æœ': r'ã€æ²»ç–—ç»“æœã€‘([^ã€]+)',
            'ç—…æ¡ˆé‡ç‚¹': r'ã€ç—…æ¡ˆé‡ç‚¹ã€‘([^ã€]+)'
        }
        
        for label, pattern in patterns.items():
            match = re.search(pattern, summary_text)
            if match:
                summary_parts.append(f"{label}ï¼š{match.group(1).strip()}")
    
    return "\n".join(summary_parts) if summary_parts else "ç—…ä¾‹æ‘˜è¦æå–å¤±è´¥"


async def crawl_case_details_improved(
    url_file: str = "iiyi_case_urls.txt",
    output_dir: str = "case_details",
    max_concurrent: int = 3,  # å‡å°‘å¹¶å‘æ•°ä»¥æé«˜æˆåŠŸç‡
    start_index: int = 0,
    end_index: Optional[int] = None,
    verbose: bool = True
) -> Dict[str, Union[int, List[str]]]:
    """
    æ”¹è¿›çš„ç—…ä¾‹è¯¦æƒ…çˆ¬å–å‡½æ•°
    
    ä½¿ç”¨ JsonCssExtractionStrategy è¿›è¡Œç»“æ„åŒ–æ•°æ®æå–
    ç›´æ¥ä¿å­˜ä¸ºJSONæ ¼å¼è€Œä¸æ˜¯markdown
    """
    
    if verbose:
        print("ğŸ” å¼€å§‹çˆ¬å–ç—…ä¾‹è¯¦æƒ…é¡µ (æ”¹è¿›ç‰ˆ)...")

    # ========== ç¬¬ä¸€é˜¶æ®µï¼šåŠ è½½URLåˆ—è¡¨ ==========
    all_urls = _load_urls_from_file(url_file)

    if end_index is None:
        end_index = len(all_urls)

    urls_to_crawl = all_urls[start_index:end_index]

    if verbose:
        print(f"ğŸ“„ æ€»è®¡ {len(all_urls)} ä¸ªURLï¼Œæœ¬æ¬¡çˆ¬å– {len(urls_to_crawl)} ä¸ª "
              f"(ç´¢å¼• {start_index} åˆ° {end_index-1})")

    # ========== ç¬¬äºŒé˜¶æ®µï¼šåˆ›å»ºè¾“å‡ºç›®å½• ==========
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # ========== ç¬¬ä¸‰é˜¶æ®µï¼šåˆ›å»ºæå–ç­–ç•¥ ==========
    # å°è¯•ä¸»schemaï¼Œå¦‚æœå¤±è´¥åˆ™å°è¯•ç®€åŒ–schema
    schemas = [get_case_extraction_schema(), get_simple_case_extraction_schema()]
    
    failed_urls: List[str] = []
    success_count = 0

    async with AsyncWebCrawler() as crawler:
        # é…ç½®markdownç”Ÿæˆå™¨ - ä½¿ç”¨å†…å®¹è¿‡æ»¤å™¨
        content_filter = _create_content_filter()
        md_generator = DefaultMarkdownGenerator(
            content_filter=content_filter,
            options={
                "ignore_links": False,
                "escape_html": False
            }
        )

        if verbose:
            print(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– (æœ€å¤§å¹¶å‘æ•°: {max_concurrent})...")

        # åˆ†æ‰¹çˆ¬å–ä»¥æ§åˆ¶å¹¶å‘
        for batch_start in range(0, len(urls_to_crawl), max_concurrent):
            batch_end = min(batch_start + max_concurrent, len(urls_to_crawl))
            batch_urls = urls_to_crawl[batch_start:batch_end]

            if verbose:
                print(f"\nğŸ“¦ æ‰¹æ¬¡ {batch_start//max_concurrent + 1}: "
                      f"çˆ¬å– {len(batch_urls)} ä¸ªURL "
                      f"({batch_start+1}-{batch_end}/{len(urls_to_crawl)})")

            # æ‰¹é‡çˆ¬å–
            results = await crawler.arun_many(batch_urls, config=None)

            # ========== ç¬¬å››é˜¶æ®µï¼šå¤„ç†ç»“æœ ==========
            for i, result in enumerate(results):
                url = batch_urls[i]
                case_id = _extract_case_id_from_url(url)

                if not result.success:
                    if verbose:
                        print(f"  âŒ å¤±è´¥: {case_id} - {result.error_message}")
                    failed_urls.append(url)
                    continue

                extracted_data = {}
                raw_markdown = ""
                
                try:
                    # å°è¯•ä½¿ç”¨ç»“æ„åŒ–æå–
                    extraction_success = False
                    for schema_idx, schema in enumerate(schemas):
                        try:
                            extraction_config = CrawlerRunConfig(
                                extraction_strategy=JsonCssExtractionStrategy(schema),
                                markdown_generator=md_generator,
                                verbose=False
                            )
                            
                            extraction_result = await crawler.arun(url, config=extraction_config)
                            
                            if extraction_result.success and hasattr(extraction_result, 'extracted_content'):
                                extracted_data = json.loads(extraction_result.extracted_content)
                                
                                extraction_success = True
                                if verbose and schema_idx > 0:
                                    print(f"  âš ï¸ ä¸»Schemaå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨SchemaæˆåŠŸ: {case_id}")
                                break
                                
                        except Exception as e:
                            if verbose and schema_idx == 0:
                                print(f"  âš ï¸ ä¸»Schemaå¤±è´¥ï¼Œå°è¯•å¤‡ç”¨Schema: {case_id} - {str(e)}")
                            continue

                    # ========== ç›´æ¥ä¿å­˜ä¸ºJSONæ ¼å¼ ==========
                    # ä¿å­˜ä¸ºJSONæ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨æå–çš„ç»“æ„åŒ–æ•°æ®
                    json_data = {
                        "url": url,
                        "case_id": case_id,
                        "extracted_data": extracted_data[0],
                        "extraction_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "extraction_success": extraction_success,
                        "data_source": "çˆ±çˆ±åŒ» (iiyi.com)"
                    }
                    
                    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
                    output_file = output_path / f"{case_id}.json"
                    with open(output_file, "w", encoding="utf-8") as f:
                        json.dump(json_data, f, ensure_ascii=False, indent=2)

                    success_count += 1

                    if verbose:
                        print(f"  âœ… æˆåŠŸ: {case_id} â†’ {output_file.name}")

                except Exception as e:
                    if verbose:
                        print(f"  âš ï¸ å¤„ç†å¤±è´¥: {case_id} - {str(e)}")
                    failed_urls.append(url)

    # ========== ç¬¬äº”é˜¶æ®µï¼šç»Ÿè®¡ä¿¡æ¯ ==========
    stats = {
        "total": len(urls_to_crawl),
        "success": success_count,
        "failed": len(failed_urls),
        "failed_urls": failed_urls
    }

    if verbose:
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬å–å®Œæˆç»Ÿè®¡ (æ”¹è¿›ç‰ˆ)")
        print("=" * 60)
        print(f"âœ… æˆåŠŸ: {stats['success']}/{stats['total']} "
              f"({stats['success']/stats['total']*100:.1f}%)")
        print(f"âŒ å¤±è´¥: {stats['failed']}/{stats['total']}")
        print(f"ğŸ“ è¾“å‡ºæ ¼å¼: JSON (ç»“æ„åŒ–æ•°æ®)")

        if failed_urls:
            print(f"\nå¤±è´¥çš„URL (å‰5ä¸ª):")
            for i, url in enumerate(failed_urls[:5], 1):
                print(f"  {i}. {url}")

    return stats


# ========== ç¤ºä¾‹ç”¨æ³• ==========

async def main_fetch_urls():
    """é‡‡é›†URLåˆ—è¡¨"""
    print("=" * 60)
    print("çˆ±çˆ±åŒ»ç—…å† URL é‡‡é›†å·¥å…·")
    print("=" * 60)

    case_urls = await fetch_all_case_urls(
        start_page=1,
        end_page=None,
        max_pages=5,  # æµ‹è¯•ç”¨ï¼Œå‡å°‘é¡µæ•°
        verbose=True
    )

    if case_urls:
        await save_case_urls_to_file(case_urls, "iiyi_case_urls.txt")
        print(f"\næ€»è®¡å‘ç°: {len(case_urls)} ä¸ªå”¯ä¸€ç—…å† URL")


async def main_crawl_details_improved():
    """æ”¹è¿›çš„ç—…ä¾‹è¯¦æƒ…çˆ¬å–"""
    print("=" * 60)
    print("çˆ±çˆ±åŒ»ç—…ä¾‹è¯¦æƒ…çˆ¬å–å·¥å…· (æ”¹è¿›ç‰ˆ)")
    print("=" * 60)

    stats = await crawl_case_details_improved(
        url_file="iiyi_case_urls.txt",
        output_dir="case_details",
        max_concurrent=3,
        start_index=0,
        end_index=3,  # æµ‹è¯•ç”¨ï¼Œåªçˆ¬å–å‰3ä¸ª
        verbose=True
    )

    print(f"\næ€»è®¡: {stats['total']} ä¸ªURL")
    print(f"æˆåŠŸ: {stats['success']} ä¸ª")
    print(f"å¤±è´¥: {stats['failed']} ä¸ª")
    print(f"ğŸ“ è¾“å‡ºæ ¼å¼: JSON (ç»“æ„åŒ–æ•°æ®)")


async def main_improved():
    """æ”¹è¿›çš„å®Œæ•´å·¥ä½œæµ"""
    print("\n" + "=" * 80)
    print(" çˆ±çˆ±åŒ»ç—…å†æ•°æ®é‡‡é›†å®Œæ•´æµç¨‹ (æ”¹è¿›ç‰ˆ)")
    print("=" * 80)

    # ç¬¬ä¸€æ­¥ï¼šé‡‡é›†URLåˆ—è¡¨
    print("\nã€ç¬¬ä¸€æ­¥ã€‘é‡‡é›†ç—…å†URLåˆ—è¡¨")
    print("-" * 80)
    await main_fetch_urls()

    # ç¬¬äºŒæ­¥ï¼šçˆ¬å–ç—…ä¾‹è¯¦æƒ…
    print("\n\nã€ç¬¬äºŒæ­¥ã€‘çˆ¬å–ç—…ä¾‹è¯¦æƒ…é¡µ (æ”¹è¿›ç‰ˆ)")
    print("-" * 80)
    await main_crawl_details_improved()

    print("\n" + "=" * 80)
    print(" å®Œæˆï¼æ‰€æœ‰ç—…å†æ•°æ®å·²ä¿å­˜åˆ° case_details/ ç›®å½• (JSONæ ¼å¼)")
    print("=" * 80)


if __name__ == "__main__":
    # è¿è¡Œæ”¹è¿›ç‰ˆå®Œæ•´æµç¨‹
    asyncio.run(main_improved())
