import json
import os
import time
import hashlib
from typing import List, Dict

# -------------------------- é…ç½® --------------------------
RAW_DATA_PATH = "medical_cases_crawl4ai/all_cases_summary.json"  # åŸå§‹æ•°æ®è·¯å¾„ï¼ˆéœ€ä¸çˆ¬è™«æ¨¡å—ä¸€è‡´ï¼‰
OUTPUT_DIR = "medical_cases_processed"  # å¤„ç†åæ•°æ®ç›®å½•
os.makedirs(OUTPUT_DIR, exist_ok=True)
REPORT_PATH = os.path.join(OUTPUT_DIR, "deduplication_report.md")  # æŠ¥å‘Šè·¯å¾„
PROCESSED_DATA_PATH = os.path.join(OUTPUT_DIR, "deduplicated_cases.json")  # å»é‡åæ•°æ®è·¯å¾„

# -------------------------- æ•°æ®å»é‡å‡½æ•° --------------------------
def deduplicate_cases(raw_cases: List[Dict]) -> tuple[List[Dict], Dict]:
    """
    å¯¹åŸå§‹ç—…ä¾‹æ•°æ®å»é‡å¹¶ç»Ÿè®¡
    è¿”å›ï¼š(å»é‡åçš„æ•°æ®, ç»Ÿè®¡æŒ‡æ ‡)
    """
    stats = {
        "raw_total": len(raw_cases),  # åŸå§‹æ•°æ®æ€»é‡
        "url_duplicate_count": 0,     # URLé‡å¤æ•°é‡
        "content_duplicate_count": 0, # å†…å®¹é‡å¤æ•°é‡
        "deduplicated_total": 0,      # å»é‡åæ•°é‡
        "total_duplicate_count": 0,   # æ€»é‡å¤æ•°é‡
        "url_duplicate_ratio": 0.0,   # URLé‡å¤å æ¯”
        "content_duplicate_ratio": 0.0, # å†…å®¹é‡å¤å æ¯”
        "deduplication_rate": 0.0     # å»é‡ç‡
    }

    crawled_urls = set()
    crawled_content_hashes = set()
    deduplicated_cases = []

    for case in raw_cases:
        case_url = case.get("case_url", "")
        
        # 1. URLå»é‡
        if not case_url or case_url in crawled_urls:
            stats["url_duplicate_count"] += 1
            continue
        
        # 2. å†…å®¹å»é‡ï¼ˆåŸºäºæ ¸å¿ƒå­—æ®µçš„å“ˆå¸Œï¼‰
        core_content = f"{case.get('case_title', '')}_{case.get('basic_info', '')}_{case.get('analysis_summary', '')}"
        content_hash = hashlib.md5(core_content.encode("utf-8")).hexdigest()
        if content_hash in crawled_content_hashes:
            stats["content_duplicate_count"] += 1
            continue
        
        # 3. æ— é‡å¤ï¼Œä¿ç•™æ•°æ®
        deduplicated_cases.append(case)
        crawled_urls.add(case_url)
        crawled_content_hashes.add(content_hash)

    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    stats["deduplicated_total"] = len(deduplicated_cases)
    stats["total_duplicate_count"] = stats["url_duplicate_count"] + stats["content_duplicate_count"]
    
    if stats["raw_total"] > 0:
        stats["url_duplicate_ratio"] = stats["url_duplicate_count"] / stats["raw_total"]
        stats["content_duplicate_ratio"] = stats["content_duplicate_count"] / stats["raw_total"]
        stats["deduplication_rate"] = stats["total_duplicate_count"] / stats["raw_total"]

    return deduplicated_cases, stats

# -------------------------- æŠ¥å‘Šç”Ÿæˆå‡½æ•° --------------------------
def generate_report(stats: Dict, output_path: str):
    """ç”Ÿæˆè¯¦ç»†çš„å»é‡åˆ†ææŠ¥å‘Š"""
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(f"""# åŒ»ç–—ç—…ä¾‹æ•°æ®å»é‡åˆ†ææŠ¥å‘Š
ç”Ÿæˆæ—¶é—´ï¼š{time.strftime("%Y-%m-%d %H:%M:%S")}

## ä¸€ã€æ•°æ®è´¨é‡æŒ‡æ ‡ï¼ˆå»é‡å‰åå¯¹æ¯”ï¼‰
| æŒ‡æ ‡                | å»é‡å‰ | å»é‡å | å˜åŒ–é‡ |
|---------------------|--------|--------|--------|
| åŸå§‹æ•°æ®æ€»é‡        | {stats['raw_total']:,}  | -      | -      |
| å»é‡åæœ‰æ•ˆæ•°æ®é‡    | -      | {stats['deduplicated_total']:,}  | -      |
| URLé‡å¤æ•°æ®é‡       | {stats['url_duplicate_count']:,}  | 0      | å‡å°‘{stats['url_duplicate_count']:,} æ¡ |
| å†…å®¹é‡å¤æ•°æ®é‡      | {stats['content_duplicate_count']:,}  | 0      | å‡å°‘{stats['content_duplicate_count']:,} æ¡ |
| æ€»é‡å¤æ•°æ®é‡        | {stats['total_duplicate_count']:,}  | 0      | å‡å°‘{stats['total_duplicate_count']:,} æ¡ |
| **æ•°æ®å»é‡ç‡**      | -      | -      | {stats['deduplication_rate']:.2%} |

## äºŒã€é‡å¤æ•°æ®åˆ†æ
- URLé‡å¤å æ¯”ï¼š{stats['url_duplicate_ratio']:.2%}ï¼ˆåŒä¸€URLå¤šæ¬¡çˆ¬å–ï¼‰
- å†…å®¹é‡å¤å æ¯”ï¼š{stats['content_duplicate_ratio']:.2%}ï¼ˆä¸åŒURLä½†å†…å®¹ä¸€è‡´ï¼‰

## ä¸‰ã€æ•°æ®è´¨é‡æå‡æ•ˆæœ
1. å®Œæ•´æ€§ï¼šæ‰€æœ‰æœ‰æ•ˆå­—æ®µï¼ˆä¸»è¯‰ã€ç°ç—…å²ç­‰ï¼‰å‡å®Œæ•´ä¿ç•™
2. å‡†ç¡®æ€§ï¼šæ¶ˆé™¤é‡å¤æ•°æ®å¯¹åç»­åˆ†æçš„å¹²æ‰°
3. å­˜å‚¨ä¼˜åŒ–ï¼šå‡å°‘ {stats['total_duplicate_count']:,} æ¡å†—ä½™æ•°æ®

## å››ã€å»é‡é…ç½®è¯´æ˜
- URLå»é‡ï¼šåŸºäºç—…ä¾‹è¯¦æƒ…é¡µURLå®Œå…¨åŒ¹é…
- å†…å®¹å»é‡ï¼šåŸºäºã€Œæ ‡é¢˜+åŸºæœ¬ä¿¡æ¯+åˆ†ææ€»ç»“ã€çš„MD5å“ˆå¸Œ
""")

# -------------------------- ä¸»å¤„ç†é€»è¾‘ --------------------------
def main():
    """è¯»å–åŸå§‹æ•°æ®ï¼Œæ‰§è¡Œå»é‡ã€ç»Ÿè®¡å’ŒæŠ¥å‘Šç”Ÿæˆ"""
    # 1. è¯»å–åŸå§‹çˆ¬å–æ•°æ®
    if not os.path.exists(RAW_DATA_PATH):
        print(f"âŒ åŸå§‹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{RAW_DATA_PATH}")
        return

    with open(RAW_DATA_PATH, "r", encoding="utf-8") as f:
        raw_cases = json.load(f)
    print(f"ğŸ“Š è¯»å–åŸå§‹æ•°æ®ï¼š{len(raw_cases)} æ¡ç—…ä¾‹")

    # 2. æ‰§è¡Œå»é‡
    deduplicated_cases, stats = deduplicate_cases(raw_cases)
    print(f"ğŸ” å»é‡å®Œæˆï¼šåŸå§‹ {stats['raw_total']} æ¡ï¼Œå»é‡å {stats['deduplicated_total']} æ¡ï¼Œå»é‡ç‡ {stats['deduplication_rate']:.2%}")

    # 3. ä¿å­˜å»é‡åçš„æ•°æ®
    with open(PROCESSED_DATA_PATH, "w", encoding="utf-8") as f:
        json.dump(deduplicated_cases, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ å»é‡åæ•°æ®ä¿å­˜è‡³ï¼š{PROCESSED_DATA_PATH}")

    # 4. ç”Ÿæˆåˆ†ææŠ¥å‘Š
    generate_report(stats, REPORT_PATH)
    print(f"ğŸ“‘ åˆ†ææŠ¥å‘Šç”Ÿæˆè‡³ï¼š{REPORT_PATH}")

if __name__ == "__main__":
    main()